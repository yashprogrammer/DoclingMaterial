{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MultiModal Extraction using Docling\n",
        "\n",
        "This notebook demonstrates how to extract multimodal page data (images, text, cells, segments) from documents and export to Parquet format.\n",
        "\n",
        "## Overview\n",
        "\n",
        "**Multimodal extraction** captures rich document information including:\n",
        "- **Page Images**: Rendered page images at configurable resolution\n",
        "- **Text Content**: Plain text and markdown representations\n",
        "- **Cells**: Structured layout cells with bounding boxes\n",
        "- **Segments**: Document segments with hierarchy\n",
        "- **Metadata**: Page dimensions, DPI, hashes, etc.\n",
        "\n",
        "## Use Cases\n",
        "\n",
        "- **Vision-Language Models**: Training data for multimodal LLMs\n",
        "- **Document Understanding**: Combined visual + textual analysis\n",
        "- **Layout Analysis**: Preserve spatial information with content\n",
        "- **Archival**: High-fidelity document preservation\n",
        "- **Dataset Creation**: Build datasets for ML training\n",
        "\n",
        "## Workflow\n",
        "\n",
        "1. Configure PDF pipeline with image generation\n",
        "2. Convert document and extract pages\n",
        "3. Generate multimodal records per page\n",
        "4. Export to Parquet format\n",
        "5. Optional: Load and visualize with HuggingFace Datasets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Installation and Setup\n",
        "\n",
        "Install required packages for multimodal extraction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "\n",
        "# Uncomment and run if packages are not installed\n",
        "\n",
        "# !pip install docling\n",
        "# !pip install docling-core\n",
        "# !pip install pandas\n",
        "# !pip install pyarrow  # Required for Parquet export\n",
        "# !pip install pillow   # For image handling\n",
        "# !pip install datasets # Optional: for loading Parquet with HuggingFace\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/yashpatil/Developer/AI/SunnySavita/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ All imports successful!\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import datetime\n",
        "import logging\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Docling imports\n",
        "from docling.datamodel.base_models import InputFormat\n",
        "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
        "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
        "from docling.utils.export import generate_multimodal_pages\n",
        "from docling.utils.utils import create_hash\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "_log = logging.getLogger(__name__)\n",
        "\n",
        "print(\"âœ“ All imports successful!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n",
        "\n",
        "Configure document processing parameters and paths.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration:\n",
            "  - Input document: /Users/yashpatil/Developer/AI/SunnySavita/sample/projectOverview.pdf\n",
            "  - Output directory: /Users/yashpatil/Developer/AI/SunnySavita/multimodal_output\n",
            "  - Image resolution scale: 2.0x (=144 DPI)\n"
          ]
        }
      ],
      "source": [
        "# Configuration parameters\n",
        "IMAGE_RESOLUTION_SCALE = 2.0  # Image scale: 1.0 = 72 DPI, 2.0 = 144 DPI, etc.\n",
        "\n",
        "# Paths\n",
        "input_doc_path = \"/Users/yashpatil/Developer/AI/SunnySavita/sample/projectOverview.pdf\"  # Change to your document\n",
        "output_dir = Path(\"/Users/yashpatil/Developer/AI/SunnySavita/multimodal_output\")\n",
        "\n",
        "# Create output directory\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Configuration:\")\n",
        "print(f\"  - Input document: {input_doc_path}\")\n",
        "print(f\"  - Output directory: {output_dir}\")\n",
        "print(f\"  - Image resolution scale: {IMAGE_RESOLUTION_SCALE}x (={IMAGE_RESOLUTION_SCALE * 72:.0f} DPI)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Initialize Document Converter with Image Generation\n",
        "\n",
        "Configure the pipeline to generate and preserve page images during conversion.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Document converter initialized\n",
            "  - Page images: ENABLED\n",
            "  - Image scale: 2.0x\n",
            "  - Table extraction: True\n"
          ]
        }
      ],
      "source": [
        "# Configure pipeline options\n",
        "# Key: generate_page_images must be True to export images\n",
        "pipeline_options = PdfPipelineOptions()\n",
        "pipeline_options.images_scale = IMAGE_RESOLUTION_SCALE\n",
        "pipeline_options.generate_page_images = True  # Critical for multimodal export\n",
        "pipeline_options.do_table_structure = True\n",
        "\n",
        "# Initialize converter\n",
        "doc_converter = DocumentConverter(\n",
        "    format_options={\n",
        "        InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"âœ“ Document converter initialized\")\n",
        "print(f\"  - Page images: ENABLED\")\n",
        "print(f\"  - Image scale: {pipeline_options.images_scale}x\")\n",
        "print(f\"  - Table extraction: {pipeline_options.do_table_structure}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Convert Document\n",
        "\n",
        "Convert the document with image generation enabled.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-10 01:29:24,042 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
            "2025-11-10 01:29:24,072 - INFO - Going to convert document batch...\n",
            "2025-11-10 01:29:24,073 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 8930b91cc2f7c6e23c0e5fd2d07fa5f5\n",
            "2025-11-10 01:29:24,087 - INFO - Loading plugin 'docling_defaults'\n",
            "2025-11-10 01:29:24,089 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
            "2025-11-10 01:29:24,091 - INFO - Loading plugin 'docling_defaults'\n",
            "2025-11-10 01:29:24,094 - INFO - Registered ocr engines: ['auto', 'easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converting document: /Users/yashpatil/Developer/AI/SunnySavita/sample/projectOverview.pdf\n",
            "This may take a moment...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-10 01:29:39,166 - INFO - Auto OCR model selected ocrmac.\n",
            "2025-11-10 01:29:39,170 - INFO - Accelerator device: 'mps'\n",
            "2025-11-10 01:29:41,103 - INFO - Accelerator device: 'mps'\n",
            "2025-11-10 01:29:41,554 - INFO - Processing document projectOverview.pdf\n",
            "2025-11-10 01:29:44,272 - INFO - Finished converting document projectOverview.pdf in 20.23 sec.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ“ Document converted in 20.23 seconds\n",
            "  - Document: projectOverview.pdf\n",
            "  - Pages: 5\n",
            "  - Hash: c6850c84582b4c487f5577356a0d9fa49847974fb387db086978d18357694fef\n"
          ]
        }
      ],
      "source": [
        "# Convert document\n",
        "print(f\"Converting document: {input_doc_path}\")\n",
        "print(\"This may take a moment...\")\n",
        "\n",
        "start_time = time.time()\n",
        "conv_res = doc_converter.convert(input_doc_path)\n",
        "conversion_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\nâœ“ Document converted in {conversion_time:.2f} seconds\")\n",
        "print(f\"  - Document: {conv_res.input.file.name}\")\n",
        "print(f\"  - Pages: {len(conv_res.document.pages)}\")\n",
        "print(f\"  - Hash: {conv_res.input.document_hash}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Generate Multimodal Records\n",
        "\n",
        "Extract multimodal data (images, text, cells, segments) for each page.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating multimodal records...\n",
            "  âœ“ Page 0: 1190x1684px @ 144 DPI\n",
            "  âœ“ Page 1: 1190x1684px @ 144 DPI\n",
            "  âœ“ Page 2: 1190x1684px @ 144 DPI\n",
            "  âœ“ Page 3: 1190x1684px @ 144 DPI\n",
            "  âœ“ Page 4: 1190x1684px @ 144 DPI\n",
            "\n",
            "âœ“ Generated 5 multimodal page records\n"
          ]
        }
      ],
      "source": [
        "# Generate multimodal records for each page\n",
        "rows = []\n",
        "\n",
        "print(\"Generating multimodal records...\")\n",
        "\n",
        "for (\n",
        "    content_text,      # Plain text content\n",
        "    content_md,        # Markdown content\n",
        "    content_dt,        # DoclingDocument content\n",
        "    page_cells,        # Layout cells\n",
        "    page_segments,     # Document segments\n",
        "    page,              # Page object with image\n",
        ") in generate_multimodal_pages(conv_res):\n",
        "    \n",
        "    # Calculate DPI from scale\n",
        "    dpi = page._default_image_scale * 72\n",
        "    \n",
        "    # Create page record\n",
        "    page_record = {\n",
        "        \"document\": conv_res.input.file.name,\n",
        "        \"hash\": conv_res.input.document_hash,\n",
        "        \"page_hash\": create_hash(\n",
        "            conv_res.input.document_hash + \":\" + str(page.page_no - 1)\n",
        "        ),\n",
        "        \"image\": {\n",
        "            \"width\": page.image.width,\n",
        "            \"height\": page.image.height,\n",
        "            \"bytes\": page.image.tobytes(),\n",
        "        },\n",
        "        \"cells\": page_cells,\n",
        "        \"contents\": content_text,\n",
        "        \"contents_md\": content_md,\n",
        "        \"contents_dt\": content_dt,\n",
        "        \"segments\": page_segments,\n",
        "        \"extra\": {\n",
        "            \"page_num\": page.page_no,\n",
        "            \"width_in_points\": page.size.width,\n",
        "            \"height_in_points\": page.size.height,\n",
        "            \"dpi\": dpi,\n",
        "        },\n",
        "    }\n",
        "    \n",
        "    rows.append(page_record)\n",
        "    print(f\"  âœ“ Page {page.page_no}: {page.image.width}x{page.image.height}px @ {dpi:.0f} DPI\")\n",
        "\n",
        "print(f\"\\nâœ“ Generated {len(rows)} multimodal page records\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Create DataFrame and Export to Parquet\n",
        "\n",
        "Convert records to a pandas DataFrame and save as Parquet file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Multimodal data exported to Parquet\n",
            "  - File: /Users/yashpatil/Developer/AI/SunnySavita/multimodal_output/multimodal_2025-11-10_013050.parquet\n",
            "  - Size: 2580.47 KB\n",
            "  - Records: 5\n",
            "\n",
            "DataFrame shape: (5, 15)\n",
            "Columns: ['document', 'hash', 'page_hash', 'cells', 'contents', 'contents_md', 'contents_dt', 'segments', 'image.width', 'image.height', 'image.bytes', 'extra.page_num', 'extra.width_in_points', 'extra.height_in_points', 'extra.dpi']\n"
          ]
        }
      ],
      "source": [
        "# Convert to DataFrame with flattened structure\n",
        "df_result = pd.json_normalize(rows)\n",
        "\n",
        "# Generate timestamped filename\n",
        "now = datetime.datetime.now()\n",
        "output_filename = output_dir / f\"multimodal_{now:%Y-%m-%d_%H%M%S}.parquet\"\n",
        "\n",
        "# Export to Parquet\n",
        "df_result.to_parquet(output_filename, engine='pyarrow')\n",
        "\n",
        "print(f\"âœ“ Multimodal data exported to Parquet\")\n",
        "print(f\"  - File: {output_filename}\")\n",
        "print(f\"  - Size: {output_filename.stat().st_size / 1024:.2f} KB\")\n",
        "print(f\"  - Records: {len(df_result)}\")\n",
        "print(f\"\\nDataFrame shape: {df_result.shape}\")\n",
        "print(f\"Columns: {list(df_result.columns)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Inspect the Data\n",
        "\n",
        "Preview the extracted multimodal data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataFrame Information:\n",
            "================================================================================\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5 entries, 0 to 4\n",
            "Data columns (total 15 columns):\n",
            " #   Column                  Non-Null Count  Dtype  \n",
            "---  ------                  --------------  -----  \n",
            " 0   document                5 non-null      object \n",
            " 1   hash                    5 non-null      object \n",
            " 2   page_hash               5 non-null      object \n",
            " 3   cells                   5 non-null      object \n",
            " 4   contents                5 non-null      object \n",
            " 5   contents_md             5 non-null      object \n",
            " 6   contents_dt             5 non-null      object \n",
            " 7   segments                5 non-null      object \n",
            " 8   image.width             5 non-null      int64  \n",
            " 9   image.height            5 non-null      int64  \n",
            " 10  image.bytes             5 non-null      object \n",
            " 11  extra.page_num          5 non-null      int64  \n",
            " 12  extra.width_in_points   5 non-null      float64\n",
            " 13  extra.height_in_points  5 non-null      float64\n",
            " 14  extra.dpi               5 non-null      float64\n",
            "dtypes: float64(3), int64(3), object(9)\n",
            "memory usage: 732.0+ bytes\n",
            "None\n",
            "\n",
            "================================================================================\n",
            "Sample Data (first row):\n",
            "================================================================================\n",
            "document: projectOverview.pdf\n",
            "hash: c6850c84582b4c487f5577356a0d9fa49847974fb387db086978d18357694fef\n",
            "page_hash: 1452e0df70b170a90e1d4eee8b139505de58e893d69a7dcfda283715db9d6aec\n",
            "cells: []\n",
            "contents: DeepWiki sunnysavita10/document_portal Menu Document Portal Overview Relevant source files Purpose and Scope This document provides a high-level introduction to the Document Portal system, a comprehen...\n",
            "contents_md: DeepWiki sunnysavita10/document_portal\n",
            "\n",
            "## Menu\n",
            "\n",
            "## Document Portal Overview\n",
            "\n",
            "Relevant source files\n",
            "\n",
            "## Purpose and Scope\n",
            "\n",
            "This document provides a high-level introduction to the Document Portal syste...\n",
            "contents_dt: <document>\n",
            "<paragraph><location><loc_7><loc_93><loc_43><loc_94></location>DeepWiki sunnysavita10/document_portal</paragraph>\n",
            "<subtitle-level-1><location><loc_9><loc_88><loc_14><loc_89></location>Menu<...\n",
            "segments: [{'index_in_doc': 0, 'label': 'text', 'text': 'DeepWiki sunnysavita10/document_portal', 'bbox': (0.06681121858580952, 0.0566063057860521, 0.4252655919926334, 0.06896377467999358), 'data': []}, {'index_in_doc': 1, 'label': 'section_header', 'text': 'Menu', 'bbox': (0.0920229991842282, 0.10777506395127684, 0.14363655642531092, 0.12063139260448939), 'data': []}, {'index_in_doc': 2, 'label': 'section_header', 'text': 'Document Portal Overview', 'bbox': (0.07626563631021652, 0.15322712682752243, 0.423492363423878, 0.1678080849342146), 'data': []}, {'index_in_doc': 3, 'label': 'text', 'text': 'Relevant source files', 'bbox': (0.12290743041729109, 0.20389583747214465, 0.30096647128630966, 0.21594567618120233), 'data': []}, {'index_in_doc': 4, 'label': 'section_header', 'text': 'Purpose and Scope', 'bbox': (0.07626563631021652, 0.26052948552405, 0.3083652884992589, 0.27638609930087804), 'data': []}, {'index_in_doc': 5, 'label': 'text', 'text': 'This document provides a high-level introduction to the Document Portal system, a comprehensive document processing platform that combines AI-powered analysis, comparison, and conversational capabilities. The system enables users to upload documents and perform various operations including metadata extraction, document comparison, and interactive question-answering through retrieval-augmented generation (RAG).', 'bbox': (0.07626563631021652, 0.30010450533155064, 0.918784516421306, 0.4047997279052215), 'data': []}, {'index_in_doc': 6, 'label': 'text', 'text': \"This overview covers the system's core capabilities, architecture, and technology stack. For detailed setup instructions, see Getting Started . For in-depth architectural details, see System Architecture . For deployment information, see Deployment and Infrastructure .\", 'bbox': (0.07626563631021652, 0.4310551921401866, 0.9219511160644673, 0.4894277227815509), 'data': []}, {'index_in_doc': 7, 'label': 'section_header', 'text': 'System Capabilities', 'bbox': (0.07626563631021652, 0.5268849641348129, 0.3162364064020852, 0.5427415779116409), 'data': []}, {'index_in_doc': 8, 'label': 'text', 'text': 'The Document Portal provides four primary document processing capabilities:', 'bbox': (0.07626563631021652, 0.5664599839423136, 0.7382799328342409, 0.5785098226513713), 'data': []}, {'index_in_doc': 9, 'label': 'table', 'text': '', 'bbox': (0.074012133374587, 0.6070841437994458, 0.9260618574976324, 0.8239280841831643), 'data': [{'html_seq': '<table><tr><th>Capability</th><th>Description</th><th>Primary Components</th></tr><tr><td>Document Analysis</td><td>Extract structured metadata from single documents using LLMs</td><td>DocumentAnalyzer , DocHandler</td></tr><tr><td>Document Comparison</td><td>Compare two documents and generate structured comparison results</td><td>DocumentComparator , DocumentComparatorLLM</td></tr><tr><td>Single Document Chat</td><td>Interactive Q&A with individual documents using RAG</td><td>SingleDocIngestor , ConversationalRAG</td></tr><tr><td>Multi Document Chat</td><td>Cross-document conversational AI with unified knowledge base</td><td>DocumentIngestor , ConversationalRAG</td></tr></table>', 'otsl_seq': ''}]}, {'index_in_doc': 10, 'label': 'text', 'text': 'The system supports multiple document formats including PDF, DOCX, and TXT files, with', 'bbox': (0.07626563631021652, 0.8622125555101172, 0.8329821039033943, 0.8742623942191748), 'data': []}, {'index_in_doc': 11, 'label': 'text', 'text': 'Ask Devin about sunnysavita10/document_portal processing powered by various LLM providers including Groq and Google GenAI.', 'bbox': (0.07437475276533512, 0.8712086598356374, 0.5271379934639772, 0.8840649884888498), 'data': []}, {'index_in_doc': 12, 'label': 'text', 'text': 'requirements.txt', 'bbox': (0.16637758252508458, 0.925787480783879, 0.3120949512424517, 0.9316003847407176), 'data': []}, {'index_in_doc': 13, 'label': 'text', 'text': 'Sources: Deep Research', 'bbox': (0.07311416373541418, 0.9236792044202933, 0.19580477283955885, 0.9443711945780551), 'data': []}, {'index_in_doc': 14, 'label': 'picture', 'text': '', 'bbox': (0.20431697115171552, 0.9262820975209759, 0.26103759156915435, 0.9488792858526998), 'data': []}, {'index_in_doc': 15, 'label': 'text', 'text': '1-22', 'bbox': (0.33352496475110754, 0.925787480783879, 0.36995262614507607, 0.9316003847407176), 'data': []}, {'index_in_doc': 16, 'label': 'text', 'text': 'README.md', 'bbox': (0.3998437132225613, 0.925787480783879, 0.4818088927333936, 0.9316003847407176), 'data': []}, {'index_in_doc': 17, 'label': 'text', 'text': '41-57', 'bbox': (0.5032506717396621, 0.925787480783879, 0.5487865090711527, 0.9316003847407176), 'data': []}, {'index_in_doc': 18, 'label': 'text', 'text': 'Share', 'bbox': (0.8092594991456555, 0.057745368903054144, 0.8559214626772088, 0.06702416042549464), 'data': []}, {'index_in_doc': 19, 'label': 'picture', 'text': '', 'bbox': (0.9008507950081988, 0.05254918219254043, 0.9241261434346784, 0.06970542011076251), 'data': []}, {'index_in_doc': 20, 'label': 'picture', 'text': '', 'bbox': (0.8983925027780559, 0.9323796686635952, 0.9178172482391969, 0.9434583848925225), 'data': []}]\n",
            "image.width: 1190\n",
            "image.height: 1684\n",
            "image.bytes: <binary data, 6011880 bytes>\n",
            "extra.page_num: 0\n",
            "extra.width_in_points: 594.9599609375\n",
            "extra.height_in_points: 841.9199829101562\n",
            "extra.dpi: 144.0\n"
          ]
        }
      ],
      "source": [
        "# Display DataFrame info\n",
        "print(\"DataFrame Information:\")\n",
        "print(\"=\" * 80)\n",
        "print(df_result.info())\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Sample Data (first row):\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Show first record (excluding image bytes for readability)\n",
        "sample = df_result.iloc[0].to_dict()\n",
        "for key, value in sample.items():\n",
        "    if key == 'image.bytes':\n",
        "        print(f\"{key}: <binary data, {len(value)} bytes>\")\n",
        "    elif isinstance(value, str) and len(value) > 200:\n",
        "        print(f\"{key}: {value[:200]}...\")\n",
        "    else:\n",
        "        print(f\"{key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Text Content Preview\n",
        "\n",
        "Display text content from the first few pages.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "Page 0\n",
            "================================================================================\n",
            "Dimensions: 595.0 x 841.9 points\n",
            "Image: 1190x1684px @ 144 DPI\n",
            "\n",
            "Text Content (first 500 chars):\n",
            "--------------------------------------------------------------------------------\n",
            "DeepWiki sunnysavita10/document_portal Menu Document Portal Overview Relevant source files Purpose and Scope This document provides a high-level introduction to the Document Portal system, a comprehensive document processing platform that combines AI-powered analysis, comparison, and conversational capabilities. The system enables users to upload documents and perform various operations including metadata extraction, document comparison, and interactive question-answering through retrieval-augme\n",
            "...\n",
            "\n",
            "================================================================================\n",
            "Page 1\n",
            "================================================================================\n",
            "Dimensions: 595.0 x 841.9 points\n",
            "Image: 1190x1684px @ 144 DPI\n",
            "\n",
            "Text Content (first 500 chars):\n",
            "--------------------------------------------------------------------------------\n",
            "High-Level Architecture System Components Overview Sources: requirements.txt 1-22 API Endpoints and Processing Flow Ask Devin about sunnysavita10/document_portal Deep Research \n"
          ]
        }
      ],
      "source": [
        "# Preview text content from first 2 pages\n",
        "num_pages_to_show = min(2, len(df_result))\n",
        "\n",
        "for i in range(num_pages_to_show):\n",
        "    row = df_result.iloc[i]\n",
        "    print(f\"\\n{'=' * 80}\")\n",
        "    print(f\"Page {row['extra.page_num']}\")\n",
        "    print(f\"{'=' * 80}\")\n",
        "    print(f\"Dimensions: {row['extra.width_in_points']:.1f} x {row['extra.height_in_points']:.1f} points\")\n",
        "    print(f\"Image: {row['image.width']}x{row['image.height']}px @ {row['extra.dpi']:.0f} DPI\")\n",
        "    print(f\"\\nText Content (first 500 chars):\")\n",
        "    print(\"-\" * 80)\n",
        "    content = row['contents']\n",
        "    print(content[:500] if len(content) > 500 else content)\n",
        "    if len(content) > 500:\n",
        "        print(\"...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "otebook "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Load with HuggingFace Datasets and reconstruct images\n",
        "# Uncomment to run (requires: pip install datasets pillow)\n",
        "\n",
        "# from datasets import Dataset\n",
        "# from PIL import Image\n",
        "# import io\n",
        "# \n",
        "# # Load the Parquet file\n",
        "# multimodal_df = pd.read_parquet(output_filename)\n",
        "# \n",
        "# # Convert to HuggingFace Dataset\n",
        "# dataset = Dataset.from_pandas(multimodal_df)\n",
        "# \n",
        "# # Function to reconstruct images from bytes\n",
        "# def reconstruct_image(example):\n",
        "#     \"\"\"Reconstruct PIL Image from raw bytes.\"\"\"\n",
        "#     img = Image.frombytes(\n",
        "#         'RGB',\n",
        "#         (example[\"image.width\"], example[\"image.height\"]),\n",
        "#         example[\"image.bytes\"],\n",
        "#         'raw'\n",
        "#     )\n",
        "#     example[\"reconstructed_image\"] = img\n",
        "#     return example\n",
        "# \n",
        "# # Apply transformation\n",
        "# dataset = dataset.map(reconstruct_image)\n",
        "# \n",
        "# print(\"âœ“ Dataset loaded with reconstructed images\")\n",
        "# print(f\"  - Records: {len(dataset)}\")\n",
        "# print(f\"  - Features: {dataset.features}\")\n",
        "# \n",
        "# # Display first image\n",
        "# first_record = dataset[0]\n",
        "# print(f\"\\nFirst page image: {first_record['reconstructed_image'].size}\")\n",
        "# first_record['reconstructed_image'].show()  # Opens image viewer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Complete Pipeline Function\n",
        "\n",
        "A reusable function to process any document and export multimodal data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Visualize Page Images (Alternative)\n",
        "\n",
        "Display page images directly from the extracted data using matplotlib.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example: Use the Pipeline Function\n",
        "\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize page images using matplotlib\n",
        "# Uncomment to display images\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "# from PIL import Image\n",
        "# import io\n",
        "# \n",
        "# # Number of pages to visualize\n",
        "# num_pages = min(3, len(df_result))\n",
        "# \n",
        "# fig, axes = plt.subplots(1, num_pages, figsize=(15, 5))\n",
        "# if num_pages == 1:\n",
        "#     axes = [axes]\n",
        "# \n",
        "# for i in range(num_pages):\n",
        "#     row = df_result.iloc[i]\n",
        "#     \n",
        "#     # Reconstruct image from bytes\n",
        "#     img = Image.frombytes(\n",
        "#         'RGB',\n",
        "#         (row['image.width'], row['image.height']),\n",
        "#         row['image.bytes'],\n",
        "#         'raw'\n",
        "#     )\n",
        "#     \n",
        "#     # Display\n",
        "#     axes[i].imshow(img)\n",
        "#     axes[i].set_title(f\"Page {row['extra.page_num']}\\n{row['image.width']}x{row['image.height']}px\")\n",
        "#     axes[i].axis('off')\n",
        "# \n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "# \n",
        "# print(f\"âœ“ Displayed {num_pages} page images\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Use Cases and Applications\n",
        "\n",
        "### Training Vision-Language Models (VLMs)\n",
        "\n",
        "The exported Parquet files are ideal for training multimodal models:\n",
        "\n",
        "```python\n",
        "# Load as HuggingFace dataset for training\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset('parquet', data_files='multimodal_*.parquet')\n",
        "\n",
        "# Use with PyTorch DataLoader\n",
        "from torch.utils.data import DataLoader\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "```\n",
        "\n",
        "### Document Understanding Pipeline\n",
        "\n",
        "Combine visual and textual features:\n",
        "\n",
        "```python\n",
        "# Extract features for each page\n",
        "for page in dataset:\n",
        "    image = page['reconstructed_image']\n",
        "    text = page['contents']\n",
        "    cells = page['cells']\n",
        "    \n",
        "    # Process with vision model\n",
        "    visual_features = vision_model(image)\n",
        "    \n",
        "    # Process with language model\n",
        "    text_features = language_model(text)\n",
        "    \n",
        "    # Combine for downstream tasks\n",
        "    combined_features = combine(visual_features, text_features)\n",
        "```\n",
        "\n",
        "### Document Archival System\n",
        "\n",
        "Preserve documents with high fidelity:\n",
        "- Full-resolution page images\n",
        "- Structured text content\n",
        "- Layout information (cells, segments)\n",
        "- Searchable metadata\n",
        "\n",
        "### Advantages\n",
        "\n",
        "- **Efficient Storage**: Parquet format with compression\n",
        "- **Fast Access**: Columnar format for quick queries\n",
        "- **Rich Metadata**: Complete document structure preserved\n",
        "- **ML-Ready**: Direct integration with ML frameworks\n",
        "- **Scalable**: Process batches of documents efficiently\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def export_multimodal_document(\n",
        "    input_path: str,\n",
        "    output_dir: Path,\n",
        "    image_scale: float = 2.0,\n",
        "    include_tables: bool = True\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Convert a document and export multimodal page data to Parquet.\n",
        "    \n",
        "    Args:\n",
        "        input_path: Path to input document (PDF, DOCX, etc.)\n",
        "        output_dir: Directory to save Parquet file\n",
        "        image_scale: Image resolution scale (1.0 = 72 DPI)\n",
        "        include_tables: Enable table structure extraction\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with export info and file path\n",
        "    \"\"\"\n",
        "    # Configure pipeline\n",
        "    pipeline_options = PdfPipelineOptions()\n",
        "    pipeline_options.images_scale = image_scale\n",
        "    pipeline_options.generate_page_images = True\n",
        "    pipeline_options.do_table_structure = include_tables\n",
        "    \n",
        "    # Initialize converter\n",
        "    doc_converter = DocumentConverter(\n",
        "        format_options={\n",
        "            InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    # Convert document\n",
        "    print(f\"Converting: {input_path}\")\n",
        "    start_time = time.time()\n",
        "    conv_res = doc_converter.convert(input_path)\n",
        "    \n",
        "    # Generate multimodal records\n",
        "    rows = []\n",
        "    for (content_text, content_md, content_dt, page_cells, \n",
        "         page_segments, page) in generate_multimodal_pages(conv_res):\n",
        "        \n",
        "        dpi = page._default_image_scale * 72\n",
        "        rows.append({\n",
        "            \"document\": conv_res.input.file.name,\n",
        "            \"hash\": conv_res.input.document_hash,\n",
        "            \"page_hash\": create_hash(\n",
        "                conv_res.input.document_hash + \":\" + str(page.page_no - 1)\n",
        "            ),\n",
        "            \"image\": {\n",
        "                \"width\": page.image.width,\n",
        "                \"height\": page.image.height,\n",
        "                \"bytes\": page.image.tobytes(),\n",
        "            },\n",
        "            \"cells\": page_cells,\n",
        "            \"contents\": content_text,\n",
        "            \"contents_md\": content_md,\n",
        "            \"contents_dt\": content_dt,\n",
        "            \"segments\": page_segments,\n",
        "            \"extra\": {\n",
        "                \"page_num\": page.page_no,\n",
        "                \"width_in_points\": page.size.width,\n",
        "                \"height_in_points\": page.size.height,\n",
        "                \"dpi\": dpi,\n",
        "            },\n",
        "        })\n",
        "    \n",
        "    # Create DataFrame and export\n",
        "    df_result = pd.json_normalize(rows)\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    now = datetime.datetime.now()\n",
        "    output_filename = output_dir / f\"multimodal_{now:%Y-%m-%d_%H%M%S}.parquet\"\n",
        "    df_result.to_parquet(output_filename, engine='pyarrow')\n",
        "    \n",
        "    elapsed = time.time() - start_time\n",
        "    \n",
        "    return {\n",
        "        \"input_path\": input_path,\n",
        "        \"output_file\": str(output_filename),\n",
        "        \"num_pages\": len(rows),\n",
        "        \"file_size_kb\": output_filename.stat().st_size / 1024,\n",
        "        \"processing_time\": elapsed,\n",
        "        \"image_dpi\": image_scale * 72,\n",
        "    }\n",
        "\n",
        "print(\"âœ“ Multimodal export function defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example usage of the pipeline function\n",
        "# Uncomment to process a document\n",
        "\n",
        "# result = export_multimodal_document(\n",
        "#     input_path=\"/path/to/your/document.pdf\",\n",
        "#     output_dir=Path(\"multimodal_output\"),\n",
        "#     image_scale=2.0,  # 144 DPI\n",
        "#     include_tables=True\n",
        "# )\n",
        "# \n",
        "# print(\"\\nExport Complete!\")\n",
        "# print(f\"  - Output file: {result['output_file']}\")\n",
        "# print(f\"  - Pages processed: {result['num_pages']}\")\n",
        "# print(f\"  - File size: {result['file_size_kb']:.2f} KB\")\n",
        "# print(f\"  - Processing time: {result['processing_time']:.2f} seconds\")\n",
        "# print(f\"  - Image DPI: {result['image_dpi']:.0f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "de "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated:\n",
        "\n",
        "1. **Setup**: Configured Docling with image generation enabled\n",
        "2. **Conversion**: Processed document with multimodal extraction\n",
        "3. **Data Generation**: Created per-page records with images, text, cells, and segments\n",
        "4. **Export**: Saved to Parquet format for efficient storage\n",
        "5. **Loading**: Showed how to load and reconstruct data with HuggingFace Datasets\n",
        "6. **Visualization**: Displayed page images and content\n",
        "7. **Pipeline**: Created reusable function for batch processing\n",
        "\n",
        "### Key Parameters\n",
        "\n",
        "- **`images_scale`**: Controls image resolution (1.0 = 72 DPI, 2.0 = 144 DPI)\n",
        "- **`generate_page_images`**: Must be `True` for multimodal export\n",
        "- **Output format**: Parquet with flattened JSON structure\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Process multiple documents in batch\n",
        "- Integrate with ML training pipelines\n",
        "- Build custom visualization tools\n",
        "- Create document search and retrieval systems\n",
        "- Train vision-language models on extracted data\n",
        "\n",
        "### Resources\n",
        "\n",
        "- **Docling Documentation**: https://github.com/DS4SD/docling\n",
        "- **Parquet Format**: https://parquet.apache.org/\n",
        "- **HuggingFace Datasets**: https://huggingface.co/docs/datasets/\n",
        "\n",
        "---\n",
        "\n",
        "**Ready for multimodal document processing! ðŸš€**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
